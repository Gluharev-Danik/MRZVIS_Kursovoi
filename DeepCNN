import os
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, WeightedRandomSampler
from torch.cuda.amp import autocast, GradScaler
from tqdm import tqdm
from sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score
import mlflow
import mlflow.pytorch
from multiprocessing import freeze_support


# -----------------------------
# 1. Фиксируем seed для воспроизводимости
# -----------------------------
def set_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


set_seed(42)

# -----------------------------
# 2. Устройство
# -----------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# -----------------------------
# 3. Пути к данным
# -----------------------------
data_dir = '.'  # текущая директория
train_dir = os.path.join(data_dir, 'train')
val_dir = os.path.join(data_dir, 'val')
test_dir = os.path.join(data_dir, 'test')

# -----------------------------
# 4. Гиперпараметры
# -----------------------------
IMG_SIZE = 150
BATCH_SIZE = 64
EPOCHS = 15
LR = 0.001
NUM_WORKERS = 0  # Изменено на 0 для Windows совместимости

# -----------------------------
# 5. Преобразования с аугментацией
# -----------------------------
transform_train = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.RandomAffine(degrees=10, shear=0.2, scale=(0.8, 1.2)),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

transform_test = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# -----------------------------
# 6. Датасеты
# -----------------------------
train_dataset = datasets.ImageFolder(root=train_dir, transform=transform_train)
val_dataset = datasets.ImageFolder(root=val_dir, transform=transform_test)
test_dataset = datasets.ImageFolder(root=test_dir, transform=transform_test)

# -----------------------------
# 7. Балансировка классов
# -----------------------------
targets = train_dataset.targets
class_counts = np.bincount(targets)
weights = 1. / class_counts
sample_weights = [weights[t] for t in targets]
sampler = torch.utils.data.WeightedRandomSampler(sample_weights, len(sample_weights))

# -----------------------------
# 8. Даталоадеры
# -----------------------------
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS,
                          pin_memory=True, sampler=sampler)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,
                        num_workers=NUM_WORKERS, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,
                         num_workers=NUM_WORKERS, pin_memory=True)

classes = train_dataset.classes

# -----------------------------
# 9. Модель (аналогично Colab)
# -----------------------------
class EnhancedCNN(nn.Module):
    def __init__(self):
        super(EnhancedCNN, self).__init__()
        self.features = nn.Sequential(
            # Stage 1
            nn.Conv2d(3, 16, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(16, 16, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),

            # Stage 2
            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),

            # Stage 3
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),

            # Dilated Layers
            nn.Conv2d(64, 96, kernel_size=3, padding=2, dilation=2),
            nn.ReLU(),
            nn.Conv2d(96, 96, kernel_size=3),  # valid
            nn.ReLU(),
            nn.MaxPool2d(2),

            nn.Conv2d(96, 128, kernel_size=3, padding=2, dilation=2),
            nn.ReLU(),
            nn.Conv2d(128, 128, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(2),
        )
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(256, 1)
        )

    def forward(self, x):
        x = self.features(x)
        return self.classifier(x)


# -----------------------------
# 10. EarlyStopping
# -----------------------------
class EarlyStopping:
    def __init__(self, patience=5, verbose=False, path='best_model.pth'):
        self.patience = patience
        self.verbose = verbose
        self.counter = 0
        self.best_loss = None
        self.early_stop = False
        self.val_loss_min = np.Inf
        self.path = path

    def __call__(self, val_loss, model):
        score = -val_loss
        if self.best_loss is None:
            self.best_loss = val_loss
            self.save_checkpoint(model)
        elif val_loss > self.best_loss:
            self.counter += 1
            if self.verbose:
                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_loss = val_loss
            self.save_checkpoint(model)
            self.counter = 0

    def save_checkpoint(self, model):
        torch.save(model.state_dict(), self.path)
        self.val_loss_min = self.best_loss


# -----------------------------
# 11. Оценка точности и потерь
# -----------------------------
def evaluate(loader, model, criterion):
    model.eval()
    correct = 0
    total = 0
    val_loss = 0
    all_preds = []
    all_labels = []
    with torch.no_grad():
        for images, labels in loader:
            images, labels = images.to(device), labels.to(device).float()
            outputs = model(images).flatten()
            loss = criterion(outputs, labels)
            val_loss += loss.item()

            preds = (torch.sigmoid(outputs) > 0.5).float()
            total += labels.size(0)
            correct += (preds == labels).sum().item()
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    acc = correct / total
    return acc, val_loss / len(loader), all_preds, all_labels


# -----------------------------
# 12. Основная функция обучения
# -----------------------------
def main():
    # Инициализация модели
    model = EnhancedCNN().to(device)

    # Loss и оптимизатор
    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=LR)

    # LR Scheduler
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)

    # AMP Scaler
    scaler = GradScaler()

    early_stopping = EarlyStopping(patience=7, verbose=True, path="best_model.pth")
    best_test_acc = 0.0

    print(f"\nОбучение начато | Эпох: {EPOCHS} | Batch Size: {BATCH_SIZE}")
    print("-" * 90)

    # -----------------------------
    # 13. Обучение модели
    # -----------------------------
    for epoch in range(EPOCHS):
        model.train()
        running_loss = 0.0
        all_train_preds = []
        all_train_labels = []

        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{EPOCHS}")
        for images, labels in progress_bar:
            images, labels = images.to(device), labels.to(device).float()

            optimizer.zero_grad()

            with autocast():
                outputs = model(images).flatten()
                loss = criterion(outputs, labels)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            running_loss += loss.item()
            progress_bar.set_postfix(loss=loss.item())

            preds = (torch.sigmoid(outputs) > 0.5).float().cpu().numpy()
            all_train_preds.extend(preds)
            all_train_labels.extend(labels.cpu().numpy())

        avg_train_loss = running_loss / len(train_loader)
        train_acc = accuracy_score(all_train_labels, all_train_preds)
        train_f1 = f1_score(all_train_labels, all_train_preds)

        # Логируем train метрики
        mlflow.log_metrics({
            "train/loss": avg_train_loss,
            "train/accuracy": train_acc,
            "train/f1": train_f1
        }, step=epoch)

        # Валидация
        val_acc, val_loss, val_preds, val_labels = evaluate(val_loader, model, criterion)
        val_f1 = f1_score(val_labels, val_preds)

        # Логируем val метрики
        mlflow.log_metrics({
            "val/loss": val_loss,
            "val/accuracy": val_acc,
            "val/f1": val_f1
        }, step=epoch)

        # Тестирование
        test_acc, test_loss, test_preds, test_labels = evaluate(test_loader, model, criterion)
        test_f1 = f1_score(test_labels, test_preds)

        print(f"Val Accuracy: {val_acc:.4f}, Val Loss: {val_loss:.4f} | "
              f"Test Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}")

        # LR и EarlyStopping по тестовым данным
        scheduler.step(test_loss)
        early_stopping(test_loss, model)

        if test_acc > best_test_acc:
            best_test_acc = test_acc
            torch.save(model.state_dict(), "best_model.pth")

        if early_stopping.early_stop:
            print("Early stopping triggered.")
            break

    # -----------------------------
    # 14. Тестирование (финальное)
    # -----------------------------
    model.load_state_dict(torch.load("best_model.pth"))
    final_test_acc, final_test_loss, test_preds, test_labels = evaluate(test_loader, model, criterion)
    final_test_f1 = f1_score(test_labels, test_preds)

    # Логируем финальные тестовые метрики
    mlflow.log_metrics({
        "test/loss": final_test_loss,
        "test/accuracy": final_test_acc,
        "test/f1": final_test_f1
    })

    print("-" * 90)
    print(f"Финальная точность на тесте: {final_test_acc:.4f}, Лосс: {final_test_loss:.4f}")
    print("-" * 90)
    print("Classification Report (Test):")
    print(classification_report(test_labels, test_preds, target_names=classes))
    print("-" * 90)

    # Сохранение модели в MLflow
    mlflow.pytorch.log_model(model, "model")


# -----------------------------
# 15. Точка входа
# -----------------------------
if __name__ == '__main__':
    # Для Windows
    import sys
    freeze_support()
    sys.exit(main())
